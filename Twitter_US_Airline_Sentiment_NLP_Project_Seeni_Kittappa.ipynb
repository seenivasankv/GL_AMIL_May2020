{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries, load dataset, print shape of data, data description.\n",
    "\n",
    "import numpy as np   \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk           \n",
    "\n",
    "import os,sys\n",
    "from scipy import stats\n",
    "\n",
    "#!pip install contractions\n",
    "# import contractions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand of data-columns: \n",
    "\n",
    "dataset = pd.read_csv(\"C:\\\\Users\\\\Seeni\\\\PG-AIML\\\\NLP\\\\Project\\Tweets.csv\")\n",
    "\n",
    "# Shape of data.\n",
    "dataset.head()\n",
    "\n",
    "# Print first 5 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "airline_sentiment\n",
      "airline_sentiment_confidence\n",
      "negativereason\n",
      "negativereason_confidence\n",
      "airline\n",
      "airline_sentiment_gold\n",
      "name\n",
      "negativereason_gold\n",
      "retweet_count\n",
      "text\n",
      "tweet_coord\n",
      "tweet_created\n",
      "tweet_location\n",
      "user_timezone\n"
     ]
    }
   ],
   "source": [
    "# print all columns to view and keep only required;\n",
    "\n",
    "for col in dataset.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other columns except “text” and “airline_sentiment” \n",
    "data = dataset.drop(['tweet_id', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'negativereason','negativereason_confidence','airline','airline_sentiment_gold','name','negativereason_gold','retweet_count','tweet_coord','tweet_created','tweet_location','user_timezone'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                                                             @VirginAmerica What @dhepburn said.  \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!  \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first 5 rows of dataframe.\n",
    "pd.set_option('display.max_colwidth', None) # It will enable the entire row visible with truncation of the text.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                                                             @VirginAmerica What @dhepburn said.  \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!  \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of the http link using Regular Expression.\n",
    "for i, row in data.iterrows():\n",
    "    clean_text = re.sub(r\"http\\S+\", \"\", data.at[i, 'text'])\n",
    "    data.at[i,'text'] = clean_text\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, What, @, dhepburn, said, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>[@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &amp;, amp, ;, they, have, little, recourse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                                                               text  \n",
       "0                                                                                                                    [@, VirginAmerica, What, @, dhepburn, said, .]  \n",
       "1                                                                        [@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]  \n",
       "2                                                                      [@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]  \n",
       "3  [@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &, amp, ;, they, have, little, recourse]  \n",
       "4                                                                                            [@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the words of whole dataframe.\n",
    "for i, row in data.iterrows():\n",
    "    text = data.at[i, 'text']\n",
    "    words = nltk.word_tokenize(text)\n",
    "    data.at[i,'text'] = words\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='airline_sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn')\n",
    "sns.countplot(data=data,x='airline_sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few additional Imports needed here ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet    # Stopwords, and wordnet corpus\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the stopwords in a list named stopwords.\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define set of new functions to normalize, which processes all the steps together whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Special Characters and Punctuations.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)        # Append processed words to new list.\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to lowercase\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = word.lower()           # Converting to lowercase\n",
    "        new_words.append(new_word)        # Append processed words to new list.\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)    # Append processed words to new list.\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words in English are “a”, “the”, “is”, “are” and etc.\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)        # Append processed words to new list.\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - Reducing inflection in words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []                            # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)                # Append processed words to new list.\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization : Proper reduction of word inflections\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []                           # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)              # Append processed words to new list.\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to execute the above functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    clean_review = remove_non_ascii(words)\n",
    "    clean_review = to_lowercase(clean_review)\n",
    "    clean_review = remove_punctuation(clean_review)\n",
    "    clean_review = remove_stopwords(clean_review)\n",
    "    clean_review = lemmatize_verbs(clean_review)\n",
    "    return clean_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, What, @, dhepburn, said, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>[@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &amp;, amp, ;, they, have, little, recourse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                                                               text  \n",
       "0                                                                                                                    [@, VirginAmerica, What, @, dhepburn, said, .]  \n",
       "1                                                                        [@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]  \n",
       "2                                                                      [@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]  \n",
       "3  [@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &, amp, ;, they, have, little, recourse]  \n",
       "4                                                                                            [@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the words in the list to convert back to text string in the dataframe.\n",
    "\n",
    "clean_review =[]\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    words = data.at[i, 'text']\n",
    "    words = normalize(words)\n",
    "    data.at[i,'text'] = \" \".join(words)\n",
    "    clean_review.append(data.at[i, 'text'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus add commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica nt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxious entertainment guests face amp little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                            text  \n",
       "0                                                                     virginamerica dhepburn say  \n",
       "1                                            virginamerica plus add commercials experience tacky  \n",
       "2                                        virginamerica nt today must mean need take another trip  \n",
       "3  virginamerica really aggressive blast obnoxious entertainment guests face amp little recourse  \n",
       "4                                                             virginamerica really big bad thing  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 5 rows of data after pre-processing.\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print (\"Creating the bag of words...\\n\")\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = None) \n",
    "                \n",
    "\n",
    "count_vectorizer_data_features = count_vectorizer.fit_transform(data)\n",
    "\n",
    "count_vectorizer_data_features = count_vectorizer_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print (count_vectorizer_data_features.shape)\n",
    "print(count_vectorizer_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airline_sentiment', 'text']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_vocab = count_vectorizer.get_feature_names()\n",
    "print (count_vectorizer_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500,min_df=7,max_df=0.8,stop_words=stopwords.words('english'))\n",
    "processed_Features = vectorizer.fit_transform(clean_review).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features = data.iloc[1,10].values\n",
    "Labels = data.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(clean_review,Labels,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Classifification Models And Error Analysis Comaparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def convert(data):\n",
    "    number = preprocessing.LabelEncoder()\n",
    "    data['airline_sentiment'] = number.fit_transform(data)\n",
    "    data['text'] = number.fit_transform(data.Source)\n",
    "    data=data.fillna(-999)\n",
    "    return data\n",
    "\n",
    "train=convert(x_train)\n",
    "test=convert(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
    "\n",
    "forest = forest.fit(x_train, y_train)\n",
    "\n",
    "print(forest)\n",
    "\n",
    "print(np.mean(cross_val_score(forest, processed_Features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "result = forest.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200,random_state=None)\n",
    "text_classifier.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Algorithm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "text_classifier2 = KNeighborsClassifier(n_neighbors = 5)#no of neighbors is hpyer parameter\n",
    "text_classifier2.fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = text_classifier2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions2))\n",
    "print(classification_report(y_test,predictions2))\n",
    "print(accuracy_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "predictions3 = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions3))\n",
    "print(classification_report(y_test,predictions3))\n",
    "print(accuracy_score(y_test, predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary :\n",
    "\n",
    "Pre-processed the data using variuos techniques and libraries.\n",
    "The pre-precessed data is converted to numbers, so that we can feed the data in the model.\n",
    "After building the classification model, we predicted the result for the test data.\n",
    "After that we saw that using the above techniques, our model performed good in perspective of how the text classification models perform.\n",
    "However, we can still increase the accuracy of our model by increasing the dataset we took into account for the model building One more way to increase accuracy is to use different variations of Pre-processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
